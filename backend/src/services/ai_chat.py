"""AI Chat Service - OpenAI/Anthropic Integration with RAG"""

import logging
from typing import List, Optional, Dict
from dataclasses import dataclass
import aiohttp

from src.core.config import settings

logger = logging.getLogger(__name__)


@dataclass
class ChatSource:
    """Source document used for RAG"""
    pmid: str
    title: str
    abstract: str
    relevance: float


@dataclass
class ChatResponse:
    """AI chat response"""
    answer: str
    sources_used: List[str]  # PMIDs
    confidence: float


class AIService:
    """AI Service for chat with RAG support"""

    def __init__(
        self,
        api_key: str = "",
        model: str = "gpt-4o-mini",
        provider: str = "openai"  # "openai" or "anthropic"
    ):
        self.api_key = api_key or settings.OPENAI_API_KEY
        self.model = model
        self.provider = provider
        self._session: Optional[aiohttp.ClientSession] = None

    async def _get_session(self) -> aiohttp.ClientSession:
        """Get or create aiohttp session"""
        if self._session is None or self._session.closed:
            self._session = aiohttp.ClientSession()
        return self._session

    async def close(self):
        """Close the session"""
        if self._session and not self._session.closed:
            await self._session.close()

    def _build_system_prompt(self) -> str:
        """Build system prompt for biomedical RAG with enhanced Chain of Thought"""
        return """You are Bio-RAG, a friendly and knowledgeable AI research assistant specialized in biomedical literature analysis.

ðŸŽ¯ **Your Mission:**
You help researchers, students, and healthcare professionals understand complex biomedical research by providing clear, thorough, and well-reasoned answers based on scientific literature.

ðŸ“š **Core Principles:**
1. **Accuracy First**: Always base your answers on the provided research papers
2. **Clear Citations**: Reference papers using PMID (e.g., "PMID:12345ì— ë”°ë¥´ë©´..." or "According to PMID:12345...")
3. **Honest Limitations**: If information is incomplete, acknowledge it openly
4. **Accessible Language**: Explain complex concepts in an understandable way while maintaining scientific accuracy

ðŸŒ **Language:**
- **IMPORTANT**: Respond in the SAME LANGUAGE as the user's question
- Korean question (í•œêµ­ì–´) â†’ Korean response
- English question â†’ English response
- Use appropriate scientific terminology with explanations when needed

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ðŸ“ **RESPONSE FORMAT (Chain of Thought - ë°˜ë“œì‹œ ë”°ë¼ì£¼ì„¸ìš”):**
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ì°¨ê·¼ì°¨ê·¼ ë‹¨ê³„ë³„ë¡œ ì‚¬ê³ í•˜ë©° ìµœì„ ì˜ ë‹µë³€ì„ ìž‘ì„±í•´ì£¼ì„¸ìš”:

---

## ðŸ” 1. ì§ˆë¬¸ ì´í•´ (Understanding the Question)

ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì„ ê¹Šì´ ì´í•´í•˜ê³  ëª…í™•í•˜ê²Œ ìž¬ì§„ìˆ í•©ë‹ˆë‹¤.
- í•µì‹¬ í‚¤ì›Œë“œì™€ ê°œë… íŒŒì•…
- ì§ˆë¬¸ì˜ ë²”ìœ„ì™€ ë§¥ë½ ì´í•´
- ì‚¬ìš©ìžê°€ ì •ë§ ì•Œê³  ì‹¶ì–´í•˜ëŠ” ê²ƒì´ ë¬´ì—‡ì¸ì§€ íŒŒì•…

---

## ðŸ’­ 2. ì‚¬ê³  ê³¼ì • (Thinking Process)

ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•œ ì ‘ê·¼ ë°©ì‹ì„ ì²´ê³„ì ìœ¼ë¡œ ìƒê°í•©ë‹ˆë‹¤.

**ðŸ§  ë¶„ì„ ê´€ì :**
- ì´ ì§ˆë¬¸ì— ë‹µí•˜ë ¤ë©´ ì–´ë–¤ ì •ë³´ê°€ í•„ìš”í•œê°€?
- ì œê³µëœ ë…¼ë¬¸ë“¤ì—ì„œ ì–´ë–¤ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ìžˆëŠ”ê°€?
- ì—¬ëŸ¬ ë…¼ë¬¸ì˜ ì •ë³´ë¥¼ ì–´ë–»ê²Œ ì¢…í•©í•  ê²ƒì¸ê°€?

**ðŸ“Š ê³ ë ¤ì‚¬í•­:**
- ì—°êµ¬ ë°©ë²•ë¡ ì˜ ì‹ ë¢°ì„±
- ê²°ê³¼ì˜ ì¼ê´€ì„± ë˜ëŠ” ìƒì¶©ì 
- ìž„ìƒì /ì‹¤ì œì  ì˜ë¯¸

---

## ðŸ”¬ 3. ë¶„ì„ ë° ê´€ì°° (Analysis & Observations)

ì œê³µëœ ë…¼ë¬¸ë“¤ì„ í•˜ë‚˜ì”© ë¶„ì„í•˜ê³  ê´€ë ¨ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
(ì´ ê³¼ì •ì€ ë…¼ë¬¸ ìˆ˜ì— ë”°ë¼ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µë©ë‹ˆë‹¤)

### ðŸ“„ ë…¼ë¬¸ ë¶„ì„ 1
- **ì¶œì²˜**: [PMID ì¸ìš©]
- **í•µì‹¬ ë°œê²¬**: [ì£¼ìš” ì—°êµ¬ ê²°ê³¼]
- **ë°©ë²•ë¡ **: [ì—°êµ¬ ë°©ë²• ê°„ëžµ ì„¤ëª…]
- **ì˜ì˜**: [ì´ ë…¼ë¬¸ì´ ì§ˆë¬¸ì— ì–´ë–»ê²Œ ê¸°ì—¬í•˜ëŠ”ì§€]

### ðŸ“„ ë…¼ë¬¸ ë¶„ì„ 2
- **ì¶œì²˜**: [PMID ì¸ìš©]
- **í•µì‹¬ ë°œê²¬**: [ì£¼ìš” ì—°êµ¬ ê²°ê³¼]
- **ì—°ê´€ì„±**: [ì²« ë²ˆì§¸ ë…¼ë¬¸ê³¼ì˜ ì—°ê²°ì  ë˜ëŠ” ì°¨ì´ì ]

(í•„ìš”ì‹œ ì¶”ê°€ ë…¼ë¬¸ ë¶„ì„ ê³„ì†...)

### ðŸ”— ì¢…í•© ê´€ì°°
- ë…¼ë¬¸ë“¤ ê°„ì˜ ê³µí†µì ê³¼ ì°¨ì´ì 
- ì „ì²´ì ì¸ ì—°êµ¬ ë™í–¥
- ë‚¨ì•„ìžˆëŠ” ë¶ˆí™•ì‹¤ì„±ì´ë‚˜ ì—°êµ¬ ê²©ì°¨

---

## âœ¨ 4. ìµœì¢… ë‹µë³€ (Final Answer)

### ðŸ“Œ í•µì‹¬ ìš”ì•½
[ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì´ê³  ëª…í™•í•œ ë‹µë³€ - 2-3ë¬¸ìž¥]

### ðŸ“– ìƒì„¸ ì„¤ëª…
[ìœ„ì˜ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ í•œ í¬ê´„ì ì¸ ì„¤ëª…]
- ì£¼ìš” ì—°êµ¬ ê²°ê³¼ë“¤ì˜ ì¢…í•©
- ì‹¤ì œì /ìž„ìƒì  ì˜ë¯¸
- PMID ì¸ìš©ì„ í†µí•œ ê·¼ê±° ì œì‹œ

### âš ï¸ ì°¸ê³ ì‚¬í•­
[í•´ë‹¹ë˜ëŠ” ê²½ìš°]
- ì—°êµ¬ì˜ í•œê³„ì 
- ì¶”ê°€ ì—°êµ¬ê°€ í•„ìš”í•œ ë¶€ë¶„
- ê°œì¸ì  ìƒí™©ì— ë”°ë¥¸ ê³ ë ¤ì‚¬í•­

### ðŸ“š ì¸ìš©ëœ ë…¼ë¬¸
- PMID:XXXXX - [ë…¼ë¬¸ ì œëª© ìš”ì•½]
- PMID:XXXXX - [ë…¼ë¬¸ ì œëª© ìš”ì•½]

---

ðŸ’¡ **Remember**:
- ì¹œê·¼í•˜ë©´ì„œë„ ì „ë¬¸ì ì¸ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”
- ë³µìž¡í•œ ê°œë…ì€ ë¹„ìœ ë‚˜ ì˜ˆì‹œë¥¼ ë“¤ì–´ ì„¤ëª…í•˜ì„¸ìš”
- ë¶ˆí™•ì‹¤í•œ ë¶€ë¶„ì€ ì†”ì§í•˜ê²Œ ì¸ì •í•˜ì„¸ìš”
- í•­ìƒ ê·¼ê±° ê¸°ë°˜ì˜ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”"""

    def _build_context_prompt(self, question: str, sources: List[ChatSource]) -> str:
        """Build context-aware prompt with paper information"""
        context_parts = ["Here are relevant research papers to help answer the question:\n"]

        for i, source in enumerate(sources, 1):
            context_parts.append(f"""
Paper {i}:
- PMID: {source.pmid}
- Title: {source.title}
- Abstract: {source.abstract}
- Relevance Score: {source.relevance:.2f}
""")

        context_parts.append(f"\nUser Question: {question}")
        context_parts.append("\nPlease provide a comprehensive answer based on the above papers, citing PMIDs where appropriate.")

        return "".join(context_parts)

    async def chat_with_context(
        self,
        question: str,
        sources: List[ChatSource],
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> ChatResponse:
        """
        Generate AI response with RAG context

        Args:
            question: User's question
            sources: List of relevant papers as context
            conversation_history: Optional previous messages

        Returns:
            ChatResponse with answer and metadata
        """
        if not self.api_key:
            logger.warning("No API key configured, using fallback response")
            return self._fallback_response(question, sources)

        try:
            if self.provider == "openai":
                return await self._openai_chat(question, sources, conversation_history)
            elif self.provider == "anthropic":
                return await self._anthropic_chat(question, sources, conversation_history)
            else:
                raise ValueError(f"Unknown provider: {self.provider}")

        except Exception as e:
            logger.error(f"AI chat error: {e}")
            return self._fallback_response(question, sources)

    async def _openai_chat(
        self,
        question: str,
        sources: List[ChatSource],
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> ChatResponse:
        """Call OpenAI API"""
        session = await self._get_session()

        messages = [
            {"role": "system", "content": self._build_system_prompt()}
        ]

        # Add conversation history if provided
        if conversation_history:
            messages.extend(conversation_history[-6:])  # Last 6 messages for context

        # Add current question with paper context
        user_message = self._build_context_prompt(question, sources)
        messages.append({"role": "user", "content": user_message})

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": 0.6,
            "max_tokens": 4000
        }

        async with session.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json=payload
        ) as response:
            if response.status != 200:
                error_text = await response.text()
                logger.error(f"OpenAI API error: {response.status} - {error_text}")
                return self._fallback_response(question, sources)

            data = await response.json()
            answer = data["choices"][0]["message"]["content"]

            # Extract which PMIDs were actually used in the response
            sources_used = [s.pmid for s in sources if s.pmid in answer]

            # Calculate confidence based on sources used and response quality
            confidence = min(0.95, 0.5 + (len(sources_used) * 0.1))

            return ChatResponse(
                answer=answer,
                sources_used=sources_used,
                confidence=confidence
            )

    async def _anthropic_chat(
        self,
        question: str,
        sources: List[ChatSource],
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> ChatResponse:
        """Call Anthropic Claude API"""
        session = await self._get_session()

        # Build messages for Claude
        messages = []

        if conversation_history:
            for msg in conversation_history[-6:]:
                messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })

        user_message = self._build_context_prompt(question, sources)
        messages.append({"role": "user", "content": user_message})

        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json",
            "anthropic-version": "2023-06-01"
        }

        payload = {
            "model": "claude-3-haiku-20240307",  # Fast and cost-effective
            "max_tokens": 4000,
            "temperature": 0.6,
            "system": self._build_system_prompt(),
            "messages": messages
        }

        async with session.post(
            "https://api.anthropic.com/v1/messages",
            headers=headers,
            json=payload
        ) as response:
            if response.status != 200:
                error_text = await response.text()
                logger.error(f"Anthropic API error: {response.status} - {error_text}")
                return self._fallback_response(question, sources)

            data = await response.json()
            answer = data["content"][0]["text"]

            sources_used = [s.pmid for s in sources if s.pmid in answer]
            confidence = min(0.95, 0.5 + (len(sources_used) * 0.1))

            return ChatResponse(
                answer=answer,
                sources_used=sources_used,
                confidence=confidence
            )

    def _fallback_response(self, question: str, sources: List[ChatSource]) -> ChatResponse:
        """Generate fallback response when AI is unavailable"""
        if not sources:
            return ChatResponse(
                answer="I couldn't find relevant papers for your question. Please try rephrasing or use different keywords.",
                sources_used=[],
                confidence=0.1
            )

        # Build a basic response from the sources
        answer_parts = [
            f"Based on my search, I found {len(sources)} relevant papers for your question about '{question[:50]}...':\n\n"
        ]

        for i, source in enumerate(sources[:3], 1):
            answer_parts.append(
                f"**{i}. {source.title}** (PMID: {source.pmid})\n"
                f"{source.abstract[:300]}...\n\n"
            )

        answer_parts.append(
            "\n*Note: AI-powered analysis is currently unavailable. "
            "Please refer to the full papers for detailed information.*"
        )

        return ChatResponse(
            answer="".join(answer_parts),
            sources_used=[s.pmid for s in sources[:3]],
            confidence=0.3
        )


# Global service instance
_ai_service: Optional[AIService] = None


def get_ai_service() -> AIService:
    """Get or create AI service instance"""
    global _ai_service
    if _ai_service is None:
        _ai_service = AIService(
            api_key=settings.OPENAI_API_KEY,
            model=settings.OPENAI_MODEL if hasattr(settings, 'OPENAI_MODEL') else "gpt-4o-mini"
        )
    return _ai_service
